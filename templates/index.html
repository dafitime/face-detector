<!DOCTYPE html>
<html>
<head>
  <title>Face Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body { margin: 0; overflow: hidden; background: black; }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }
    #video { z-index: 1; }
    #canvas { z-index: 2; pointer-events: none; }
    #result {
      position: absolute;
      top: 10px;
      left: 10px;
      color: white;
      font-size: 18px;
      z-index: 3;
      font-family: sans-serif;
    }
    #toggleBtn {
      position: absolute;
      top: 10px;
      right: 10px;
      z-index: 3;
      font-size: 18px;
    }
  </style>
</head>
<body>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>
  <p id="result">‚è≥ Starting camera...</p>
  <button id="toggleBtn" onclick="switchCamera()">üîÑ Switch Camera</button>

  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const result = document.getElementById("result");

    let lastBoxes = [];
    let lastDetected = 0;
    let lastDetectionTime = 0;
    let usingRear = true;
    let stream = null;
    const detectionInterval = 150;

    startCamera();

    function startCamera() {
      const constraints = {
        video: {
          facingMode: usingRear ? { exact: "environment" } : "user"
        }
      };

      navigator.mediaDevices.getUserMedia(constraints)
        .then((newStream) => {
          stream = newStream;
          video.srcObject = newStream;

          video.addEventListener("loadedmetadata", () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            requestAnimationFrame(renderLoop);
            setTimeout(detectLoop, 500);
            result.textContent = "‚úÖ Camera ready";
          });
        })
        .catch((err) => {
          console.error("Camera error:", err);
          result.textContent = "‚ö†Ô∏è Camera access failed.";
        });
    }

    function switchCamera() {
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }
      usingRear = !usingRear;
      startCamera();
    }

    function drawBox(box) {
      ctx.beginPath();
      ctx.lineWidth = 3;
      ctx.strokeStyle = "lime";
      ctx.rect(box.x, box.y, box.w, box.h);
      ctx.stroke();

      ctx.font = "16px sans-serif";
      ctx.fillStyle = "lime";
      ctx.fillText("Face", box.x + 4, box.y - 8);
    }

    function renderLoop() {
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      lastBoxes.forEach(drawBox);
      requestAnimationFrame(renderLoop);
    }

    function detectLoop() {
      const now = Date.now();
      if (now - lastDetectionTime < detectionInterval) {
        setTimeout(detectLoop, 10);
        return;
      }
      lastDetectionTime = now;

      const dataURL = canvas.toDataURL("image/jpeg");

      fetch("/detect", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ image: dataURL })
      })
      .then(res => res.json())
      .then(data => {
        if (data.boxes?.length > 0) {
          lastBoxes = data.boxes;
          lastDetected = Date.now();
          result.textContent = "üòä Face Detected!";
        } else {
          const elapsed = Date.now() - lastDetected;
          if (elapsed > 1000) {
            lastBoxes = [];
            result.textContent = "üö´ No Face.";
          } else {
            result.textContent = "üòä (Cached) Face Detected!";
          }
        }
        setTimeout(detectLoop, 10);
      })
      .catch(err => {
        console.error("Detection error:", err);
        result.textContent = "‚ö†Ô∏è Detection error.";
        setTimeout(detectLoop, 1000);
      });
    }
  </script>
</body>
</html>
